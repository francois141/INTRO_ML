{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fP9Rei3T0ybK"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1GpCYli0ybP"
   },
   "outputs": [],
   "source": [
    "# Classic stuff\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Keras imports\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras import *\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check available devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVLW2P3i0ybS",
    "outputId": "0859cddb-538d-4ae0-d2f1-4cc168eb47ab"
   },
   "outputs": [],
   "source": [
    "# Check if gpu is available\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rvcd9J780ybT"
   },
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oJi-6G80ybU"
   },
   "outputs": [],
   "source": [
    "# Dimensions of the image\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "\n",
    "# Some parameters for the training and inference phase\n",
    "STEPS_EPOCHS = 930\n",
    "EPOCHS = 1\n",
    "STEPS_PREDICT = 1861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuVkHeaD0ybV"
   },
   "source": [
    "## Creation of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQCCaOxs0ybV"
   },
   "outputs": [],
   "source": [
    "# Load and return an image\n",
    "def load_image(img, training):\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32) / 127.5 - 1\n",
    "    return tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "\n",
    "# Load the triplets\n",
    "def load_triplets(triplet, training):\n",
    "    ids = tf.strings.split(triplet)\n",
    "    paths = ['food/' + ids[0] + '.jpg','food/' + ids[1] + '.jpg','food/' + ids[2] + '.jpg']\n",
    "    anchor = load_image(tf.io.read_file(paths[0]), training)\n",
    "    truthy = load_image(tf.io.read_file(paths[1]), training)\n",
    "    falsy = load_image(tf.io.read_file(paths[2]), training)\n",
    "    return tf.stack([anchor, truthy, falsy], axis=0),1\n",
    "    \n",
    "# Create the dataset using the input path\n",
    "def create_dataset(dataset_filename):\n",
    "    dataset = tf.data.TextLineDataset(dataset_filename)\n",
    "    dataset = dataset.map(lambda triplet: load_triplets(triplet,True))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfcrM6F90ybW"
   },
   "source": [
    "## Custom loss & accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7v9j94x30ybX"
   },
   "outputs": [],
   "source": [
    "# Custom triplet loss\n",
    "def triplet_loss(_,predictions): \n",
    "    anchor, correct, wrong = predictions[...,0],predictions[...,1],predictions[...,2]\n",
    "    distance_correct = tf.reduce_sum(tf.square(anchor - correct),axis=1)\n",
    "    distance_false = tf.reduce_sum(tf.square(anchor - wrong),axis=1)\n",
    "    return tf.reduce_mean(tf.math.softplus(distance_correct - distance_false))\n",
    "   \n",
    "# Custom function to compute accuracy\n",
    "def accuracy(_,predictions):\n",
    "    anchor, correct, wrong = predictions[...,0],predictions[...,1],predictions[...,2]\n",
    "    distance_correct = tf.reduce_sum(tf.square(anchor - correct),axis=1)\n",
    "    distance_false = tf.reduce_sum(tf.square(anchor - wrong),axis=1)\n",
    "    return tf.reduce_mean(tf.cast(tf.greater_equal(distance_false,distance_correct), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tm8l_lG60ybY"
   },
   "source": [
    "## Creation of the siamese nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gB1DZzFs0ybY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creation of the model\n",
    "def create_model():\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(3,IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    \n",
    "    # Split the layer into 3 parts\n",
    "    anchor = input_layer[:,0,...]\n",
    "    correct = input_layer[:,1,...]\n",
    "    wrong = input_layer[:,2,...]\n",
    "    \n",
    "    # Use a pretrained encoder\n",
    "    model_url = \"https://tfhub.dev/google/bit/m-r50x1/1\"\n",
    "    encoder = hub.KerasLayer(model_url, trainable=False)\n",
    "    \n",
    "    # Create a custom classificator\n",
    "    decoder = Sequential([\n",
    "        Dropout(0.3),\n",
    "        Dense(256),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(128),\n",
    "        Activation('sigmoid')       \n",
    "    ])\n",
    "    \n",
    "    # Create the output layer\n",
    "    output_layer = tf.stack([decoder(encoder(anchor)),decoder(encoder(correct)),decoder(encoder(wrong))],axis=-1)\n",
    "    \n",
    "    # Create & compile the model\n",
    "    model = Model(inputs=input_layer,outputs=output_layer)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss = triplet_loss,\n",
    "                   metrics=[accuracy])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a model object\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpaFQopQ0ybZ"
   },
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G9wd5060yba"
   },
   "outputs": [],
   "source": [
    "# Read the test data\n",
    "def getTestData(dataset_filename):\n",
    "    dataset = tf.data.TextLineDataset(dataset_filename)\n",
    "    dataset = dataset.map(lambda triplet: load_triplets(triplet,False))\n",
    "    dataset = dataset.batch(32).prefetch(10)\n",
    "    return dataset\n",
    "\n",
    "# Create a model for prediction\n",
    "def create_inference_model(model):\n",
    "    anchor, correct, wrong = model.output[...,0],model.output[...,1],model.output[...,2]\n",
    "    distance_correct = tf.reduce_sum(tf.square(anchor - correct),1)\n",
    "    distance_false = tf.reduce_sum(tf.square(anchor - wrong),1)\n",
    "    predictions = tf.cast(tf.greater_equal(distance_false,distance_correct), tf.int8)\n",
    "    return tf.keras.Model(inputs=model.inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1aeju3z0yba"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c2hxx5K0ybb"
   },
   "outputs": [],
   "source": [
    "# Load the train dataset\n",
    "train_dataset = create_dataset('train_triplets.txt')\n",
    "train_dataset = train_dataset.shuffle(1024, reshuffle_each_iteration=True).repeat().batch(64)\n",
    "\n",
    "# Fit using this dataset\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch= STEPS_EPOCHS,\n",
    "    epochs=EPOCHS, # One epochs is enough\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5jv29XS0ybc"
   },
   "source": [
    "## Output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FLwLaj90ybc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the inference model\n",
    "inference_model = create_inference_model(model)\n",
    "\n",
    "# Load the data\n",
    "input_data = getTestData('test_triplets.txt')\n",
    "\n",
    "# Do the predictions\n",
    "output = inference_model.predict(input_data,steps=STEPS_PREDICT)\n",
    "\n",
    "# Output the predictions\n",
    "np.savetxt('predictions.txt',output,fmt='%i')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Task_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
