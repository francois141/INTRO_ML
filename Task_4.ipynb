{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Task_4.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP9Rei3T0ybK"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1GpCYli0ybP"
      },
      "source": [
        "from PIL import Image\n",
        "from keras.layers import *\n",
        "from keras.applications import *\n",
        "from keras import *\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVLW2P3i0ybS",
        "outputId": "0859cddb-538d-4ae0-d2f1-4cc168eb47ab"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 14197080870678910413\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvcd9J780ybT"
      },
      "source": [
        "## Global parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oJi-6G80ybU"
      },
      "source": [
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "\n",
        "STEPS_EPOCHS = 930\n",
        "EPOCHS = 1\n",
        "\n",
        "STEPS_PREDICT = 1861"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuVkHeaD0ybV"
      },
      "source": [
        "## Creation of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQCCaOxs0ybV"
      },
      "source": [
        "def load_image(img, training):\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.cast(img, tf.float32) / 127.5 - 1\n",
        "    return tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
        "\n",
        "\n",
        "def load_triplets(triplet, training):\n",
        "    ids = tf.strings.split(triplet)\n",
        "    paths = ['food/' + ids[0] + '.jpg','food/' + ids[1] + '.jpg','food/' + ids[2] + '.jpg']\n",
        "    anchor = load_image(tf.io.read_file(paths[0]), training)\n",
        "    truthy = load_image(tf.io.read_file(paths[1]), training)\n",
        "    falsy = load_image(tf.io.read_file(paths[2]), training)\n",
        "    return tf.stack([anchor, truthy, falsy], axis=0),1\n",
        "    \n",
        "    \n",
        "def create_dataset(dataset_filename):\n",
        "    dataset = tf.data.TextLineDataset(dataset_filename)\n",
        "    dataset = dataset.map(lambda triplet: load_triplets(triplet,True))\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfcrM6F90ybW"
      },
      "source": [
        "## Custom loss & accuracy function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v9j94x30ybX"
      },
      "source": [
        "def triplet_loss(_,predictions): # we use a triplet loss in this case like for facenet\n",
        "    anchor, correct, wrong = predictions[...,0],predictions[...,1],predictions[...,2]\n",
        "    distance_correct = tf.reduce_sum(tf.square(anchor - correct),axis=1)\n",
        "    distance_false = tf.reduce_sum(tf.square(anchor - wrong),axis=1)\n",
        "    return tf.reduce_mean(tf.math.softplus(distance_correct - distance_false))\n",
        "   \n",
        "\n",
        "def accuracy(_,predictions):\n",
        "    anchor, correct, wrong = predictions[...,0],predictions[...,1],predictions[...,2]\n",
        "    distance_correct = tf.reduce_sum(tf.square(anchor - correct),axis=1)\n",
        "    distance_false = tf.reduce_sum(tf.square(anchor - wrong),axis=1)\n",
        "    return tf.reduce_mean(tf.cast(tf.greater_equal(distance_false,distance_correct), tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm8l_lG60ybY"
      },
      "source": [
        "## Creation of the siamese nn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "gB1DZzFs0ybY"
      },
      "source": [
        "def create_model():\n",
        "    input_layer = Input(shape=(3,IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "    \n",
        "    anchor = input_layer[:,0,...]\n",
        "    correct = input_layer[:,1,...]\n",
        "    wrong = input_layer[:,2,...]\n",
        "    \n",
        "    model_url = \"https://tfhub.dev/google/bit/m-r50x1/1\"\n",
        "    encoder = hub.KerasLayer(model_url, trainable=False)\n",
        "    \n",
        "    decoder = Sequential([\n",
        "        Dropout(0.3),\n",
        "        Dense(256),\n",
        "        Activation('relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(128),\n",
        "        Activation('sigmoid')       \n",
        "    ])\n",
        "    \n",
        "    output_layer = tf.stack([decoder(encoder(anchor)),decoder(encoder(correct)),decoder(encoder(wrong))],axis=-1)\n",
        "    \n",
        "    model = Model(inputs=input_layer,outputs=output_layer)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  loss = triplet_loss,\n",
        "                   metrics=[accuracy])\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpaFQopQ0ybZ"
      },
      "source": [
        "## Predictior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G9wd5060yba"
      },
      "source": [
        "def getTestData(dataset_filename):\n",
        "    dataset = tf.data.TextLineDataset(dataset_filename)\n",
        "    dataset = dataset.map(lambda triplet: load_triplets(triplet,False))\n",
        "    dataset = dataset.batch(32).prefetch(10)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def predictor(model):\n",
        "    anchor, correct, wrong = model.output[...,0],model.output[...,1],model.output[...,2]\n",
        "    distance_correct = tf.reduce_sum(tf.square(anchor - correct),1)\n",
        "    distance_false = tf.reduce_sum(tf.square(anchor - wrong),1)\n",
        "    predictions = tf.cast(tf.greater_equal(distance_false,distance_correct), tf.int8)\n",
        "    return tf.keras.Model(inputs=model.inputs, outputs=predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1aeju3z0yba"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c2hxx5K0ybb"
      },
      "source": [
        "train_dataset = create_dataset('train_triplets.txt')\n",
        "train_dataset = train_dataset.shuffle(1024, reshuffle_each_iteration=True).repeat().batch(64)\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch= STEPS_EPOCHS,\n",
        "    epochs=EPOCHS, # One epochs is enough\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5jv29XS0ybc"
      },
      "source": [
        "## Output model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0FLwLaj90ybc"
      },
      "source": [
        "inference_model = predictor(model)\n",
        "\n",
        "input_data = getTestData('test_triplets.txt')\n",
        "\n",
        "output = inference_model.predict(input_data,steps=STEPS_PREDICT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5WHtwbX0ybd"
      },
      "source": [
        "np.savetxt('predictions.txt',output,fmt='%i')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYJLmsZP0ybe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XdXPxc60ybe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}