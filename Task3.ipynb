{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ATxbZ63wEMZ"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAro6yjSwEMa"
   },
   "outputs": [],
   "source": [
    "# Basic stuff\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Specific machine learning packages\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import of some keras packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvLwMJM8wEMb"
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_GT-NT2wEMc"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data = np.array(pd.read_csv('train.csv'))[:,0]\n",
    "train_solution = np.array(pd.read_csv('train.csv'))[:,1]\n",
    "test_data = np.array(pd.read_csv('test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDCt_l7KwEMd"
   },
   "source": [
    "# Preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1aorGhywEMd"
   },
   "source": [
    "### Split & One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNhcw34FwEMd"
   },
   "outputs": [],
   "source": [
    "# This function will split the char\n",
    "def split(listwords):\n",
    "    length = listwords.shape[0]\n",
    "    output = []\n",
    "    for i in range(0,length):\n",
    "        output.append(np.array([char for char in listwords[i]]))\n",
    "    \n",
    "    return np.array(output)\n",
    "\n",
    "# Classical onehot encoding\n",
    "def onehotencode(data):\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    return encoder.fit_transform(data)\n",
    "\n",
    "# Preprocessing of the input data\n",
    "encoded_train_X = onehotencode(split(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKUemP-XwEMf"
   },
   "source": [
    "### Split for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akzZh39VwEMf",
    "outputId": "2fe795e4-f40a-422b-e699-efb0c8f72a16",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset to validate the training and avoid overfitting\n",
    "X_train,X_test,y_train,y_test = train_test_split(encoded_train_X,train_solution,test_size = 0.0001, random_state = 42)\n",
    "X_train = encoded_train_X\n",
    "y_train = train_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6rRh58rwEMf"
   },
   "source": [
    "# Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piAAChyJ0CHY"
   },
   "outputs": [],
   "source": [
    "## source code\n",
    "#https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n",
    "def f1_weighted(true, pred):\n",
    "    ground_positives = K.sum(true, axis=0) + K.epsilon()       # = TP + FN\n",
    "    pred_positives = K.sum(pred, axis=0) + K.epsilon()         # = TP + FP\n",
    "    true_positives = K.sum(true * pred, axis=0) + K.epsilon()  # = TP\n",
    "    \n",
    "    precision = true_positives / pred_positives \n",
    "    recall = true_positives / ground_positives\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "    weighted_f1 = f1 * ground_positives / K.sum(ground_positives) \n",
    "    weighted_f1 = K.sum(weighted_f1)\n",
    "\n",
    "    return weighted_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5K9iRDN00CHa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model declaration\n",
    "model = Sequential()\n",
    "model.add(Dense(150,input_shape=X_train[0].shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.109))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=[f1_weighted])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "565E4l8H0CHc",
    "outputId": "0e8a7a09-afe9-4a14-f399-439094caf47a"
   },
   "outputs": [],
   "source": [
    "# Define some customs parameters\n",
    "batch_size = 64\n",
    "epochs = 300\n",
    "weights = {0: 1, 1: 5}\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train.astype('int32'), y_train.astype('int32'),\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  verbose=True,\n",
    "                  class_weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model to predict some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rKIKaTN0CHe",
    "outputId": "81c98def-d245-4cf7-87db-ee1255b853a8"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test.astype('int32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9m4nSb7EwEMg"
   },
   "source": [
    "## Output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRZx4NCWwEMg",
    "outputId": "54abff2b-71c1-40fe-e956-187a04282f79",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare the output data\n",
    "output = model.predict_classes(onehotencode(split(test_data[:,0])))\n",
    "\n",
    "# Output it\n",
    "data = pd.Series(output.reshape(-1))\n",
    "data.to_csv(\"output.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7jM9wbJ0CHk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Task3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
